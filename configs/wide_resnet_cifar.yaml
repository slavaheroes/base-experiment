# general hyperparameters
project: base-experiments
name: wide_resnet_cifar_ddp
max_epochs: 10

train_strategy: ddp

# dataset configuration
dataset:
  lib: dataset.cifar10
  name: cifar10
  batch_size: 128
  num_workers: 8
  data_dir: /mnt
  transforms: basic

# model configuration
model:
  lib: models.wide_resnet
  name: WideResNet
  args:
    in_channels: 3
    num_classes: 10
    weight_stardardization: False
    deep_factor: 8
    width_factor: 10


# optimizer and scheduler configuration
optimizer:
  lib: torch.optim
  name: AdamW
  args:
    lr: 0.001
    betas: [0.9, 0.999]
    weight_decay: 0.0005

scheduler:
  lib: torch.optim.lr_scheduler
  name: CosineAnnealingLR
  args:
    T_max: 50
    eta_min: 0.0000001


# Ligthning callbacks

EarlyStopping:
  enable: True
  args:
    monitor: avg_valid_loss
    patience: 3
    mode: min
    verbose: True

ModelCheckpoint:
  enable: True
  dirpath: checkpoints
  filename: '{epoch}-{epoch}-{avg_valid_loss:.4f}'
  args:
    monitor: avg_valid_loss
    mode: min
    save_top_k: 2
    verbose: True
    save_last: True

# Lightning Trainer configuration
trainer:
  accumulate_grad_batches: 1
  log_every_n_steps: 1
  val_check_interval: 0.5 # twice in one epoch
  num_sanity_val_steps: 0
  gradient_clip_val: Null
  limit_train_batches: 0.25 # might be useful for debugging
  limit_val_batches: 0.5
  overfit_batches: 0.0
