
# general hyperparameters
batch_size: 128
max_epochs: 50
base_learning_rate: 0.001
num_workers: 8

# optimizer and scheduler configuration
optimizer:
  name: AdamW
  weight_decay: 0.0005
  args:
    betas: [0.9, 0.999]

scheduler:
  name: CosineAnnealingLR
  args:
    T_max: 50
    eta_min: 1e-7


# Ligthning callbacks

EarlyStopping:
  enable: True
  args:
    monitor: val_loss
    patience: 10
    mode: min
    verbose: True

ModelCheckpoint:
  enable: False
  args:
    monitor: val_loss
    mode: min
    save_top_k: 1
    verbose: True
    save_last: True

# Trainer configuration
trainer:
  accelerator: gpu
  devices: [0]
  accumulate_grad_batches: 1
  log_every_n_steps: 1
  val_check_interval: 0.5 # twice in one epoch
  num_sanity_val_steps: 0
  gradient_clip_val: None
  limit_train_batches: 1.0 # might be useful for debugging
  limit_val_batches: 1.0 
  overfit_batches: 0.0

  



